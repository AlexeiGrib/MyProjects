{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<a id='0'></a> \n# Описание проекта \"Predict TripAdvisor Rating\".\n\nВ рамках данного проекта требуется предсказать рейтинг ресторана в TripAdvisor. Работу над проектом будем проводить по следующим этапам:\n\n1. Импорт библиотек, подготовка функций, чтение и первичный анализ данных:\n\n<a href='#1.1'>1.1. Импорт библиотек, подготовка функций.</a> \n\n<a href='#1.2'>1.2. Чтение и первичный анализ данных.</a> \n\n2. Предобработка данных:\n\n<a href='#2.1'>2.1. Обработка признака \"cuisine_style\".</a> \n\n<a href='#2.2'>2.2. Обработка признака \"price_range\".</a> \n\n<a href='#2.3'>2.3. Обработка признака \"number_of_reviews\".</a> \n\n3. Исследовательский анализ данных:\n\n<a href='#3.1'>3.1. Признак \"restaurant_id\".</a> \n\n<a href='#3.2'>3.2. Признак \"ranking\".</a> \n\n<a href='#3.3'>3.3. Признак \"city\".</a> \n\n<a href='#3.4'>3.4. Признак \"rating\".</a> \n\n4. Создание новых признаков:\n\n<a href='#4.1'>4.1. Признак \"cuisine_style\".</a> \n\n<a href='#4.2'>4.2. Признак \"restaurant_id\".</a> \n\n<a href='#4.3'>4.3. Признак \"reviews\".</a> \n\n<a href='#4.4'>4.4. Признак \"city\".</a> \n\n<a href='#4.5'>4.5. Признаки \"ranking\" и \"number_of_reviews\".</a> \n\n<a href='#4.6'>4.6. Отбор признаков.</a> \n\n5. Препроцессинг:\n\n<a href='#5.1'>5.1. Написание функции для предобработки данных и создания новых признаков.</a> \n\n6. Обучение и тестирование модели:\n\n<a href='#6.1'>6.1. Обучение и тестирование модели.</a> \n\nМетрикой качества модели будет средняя квадратическая ошибка (МАЕ).","metadata":{}},{"cell_type":"markdown","source":"# Этап 1. Импорт библиотек, подготовка функций, чтение и первичный анализ данных.","metadata":{}},{"cell_type":"markdown","source":"<a id='1.1'></a> \n## Этап 1.1. Импорт библиотек, подготовка функций.\n\nИмпортируем нужные для работы библиотеки.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nimport os\nfrom datetime import date\nfrom sklearn.preprocessing import StandardScaler\npd.options.display.max_columns = 999","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Подготовим несколько функций, которые пригодятся при обработке данных.","metadata":{}},{"cell_type":"code","source":"def df_info(df):\n    print('Первые 10 строк набора данных')\n    display(df.head(10))\n    print()\n    print('Информация о наборе данных:')\n    print(df.info())\n    print()\n    print('Размер набор данных: {} признаков, {} объектов'.format(df.shape[1], df.shape[0]))\n    print('Дубликаты:', df.duplicated().sum())\n    print('Пропуски:')\n    display(df.isna().sum())\n    for i in df.columns:\n        print(df[i].value_counts())\n        \ndef date_processing(row):\n    if row['dates_of_reviews_count'] == 1:\n        row['first_review'] = pd.to_datetime(row['dates_of_reviews'][0])\n        row['second_review'] = pd.to_datetime(row['dates_of_reviews'][0])\n    \n    elif row['dates_of_reviews_count'] == 0:\n        #добавим 2 идентичные даты, чтобы признак не был пропущенным и чтобы можно было посчитать разницу для других признаков\n        row['first_review'] = date(2010, 1, 1)\n        row['second_review'] = date(2010, 1, 1)\n    \n    else:\n        if pd.to_datetime(row['dates_of_reviews'][0]) < pd.to_datetime(row['dates_of_reviews'][1]):\n            row['first_review'] = pd.to_datetime(row['dates_of_reviews'][1])\n            row['second_review'] = pd.to_datetime(row['dates_of_reviews'][0])\n        else:\n            row['first_review'] = pd.to_datetime(row['dates_of_reviews'][0])\n            row['second_review'] = pd.to_datetime(row['dates_of_reviews'][1])\n    return row                      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"С помощью системной библиотеки os получим директорию, хранящую файлы с данными.","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nRANDOM_SEED = 42\n!pip freeze > requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='1.2'></a> \n## Этап 1.2. Чтение и первичный анализ данных.\n\nСчитаем данные, используя ранее полученные пути к их местоположению.","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(data_dir + '/main_task.csv')\ndf_test = pd.read_csv(data_dir + 'kaggle_task.csv')\nsample_submission = pd.read_csv(data_dir + '/sample_submission.csv')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Для корректной обработки данных соединим тренировочный и тестовый наборы данных в единый набор, при этом тренировочный набор отметим меткой 1, тестовый - меткой 0.\n\nТакже в тестовом наборе отсутствует целевая переменная Rating, поэтому в тетсовом наборе данных создадим такой признак и заполним его константой. После обработки данных мы его удалим.","metadata":{}},{"cell_type":"code","source":"df_train['sample'] = 1\ndf_test['sample'] = 0\ndf_test['Rating'] = 0 \ndf = df_test.append(df_train, sort=False).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Применим ранее подготовленную функцию для проведения мини-EDA набора данных.","metadata":{}},{"cell_type":"code","source":"df_info(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Описание признаков:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана","metadata":{}},{"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки.\n\nДля удобства обращения к признакам сведём их к нижнему регистру, а также заменим пробелы на нижние подчёркивания.","metadata":{}},{"cell_type":"code","source":"df.columns = [str(i).lower().replace(' ', '_') for i in df.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Вывод по этапу 1.\n\nНа данном этапе мы испортировали необходимые для работы библиотеки, подготовили несколько функций для анализа и обработки данных, а также провели мини-EDA набора данных.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Этап 2. Предобработка данных.\n\nНа данном этапе осуществим предварительную обработку данных.","metadata":{}},{"cell_type":"markdown","source":"<a id='2.1'></a> \n## Этап 2.1. Признак \"cuisine_style\".\n\nВ признаке cuisine_style представлена информация о кухнях, подаваемых в рестоане.\n\nВ данном признаке есть 2 глобальных затруднения:\n\n1. Данные представлены в виде списка, но в форме строки. С такими данными довольно проблематично работать \"из коробки\".\n\n2. В данных есть пропуски.\n\nПервую проблему решать не будем: как фактор для обучения модели данный признак неинформативен, однако с ним ещё предстоит поработать на этапе создания новых признаков.\n\nПропущенные значения заменим на наиболее часто встречающуюся кухню в наборе данных.\n\nЧтобы это сделать, сначала выделим из строки каждую отдельную кухню.","metadata":{}},{"cell_type":"code","source":"top_cuisine = df[['cuisine_style']]\ntop_cuisine['cuisine_style'] = top_cuisine['cuisine_style'].str.split(',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее с помощью функции explode развернём списки с наименованиями кухонь.","metadata":{}},{"cell_type":"code","source":"top_cuisine = top_cuisine['cuisine_style'].explode()\ntop_cuisine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что список удалось успешно раскрыть, однако данные содержат много мусора. Их нужно очистить.","metadata":{}},{"cell_type":"code","source":"top_cuisine = top_cuisine.apply(lambda x: str(x).replace('[', '').replace(']', '').replace(\"'\", \"\").strip())\ntop_cuisine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь выясним, какая кухня является наиболее часто встречаемой.","metadata":{}},{"cell_type":"code","source":"top_cuisine = top_cuisine.value_counts(ascending=False).index[0]\ntop_cuisine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что кухня Vegetarian Friendly присутствует в большинстве исследуемых ресторанов. Заменим пропущенные значения признака cuisine_style на значение Vegetarian Friendly.","metadata":{}},{"cell_type":"code","source":"df['cuisine_style'] = df['cuisine_style'].fillna(top_cuisine)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cuisine_style'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пропуски в признаке \"cuisine_style\" успешно обработаны.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='2.2'></a> \n## Этап 2.2. Признак \"price_range\".\n\nВ признаке price_range также присутствуют пропуски. Их - как и в случае с кухнями - заменим на значение наиболее часто встречающегося значения ценового сегмента ресторана.","metadata":{}},{"cell_type":"code","source":"df['price_range'].value_counts(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что большинство ресторанов находятся в среднем ценовом сегменте. Заменим пропуски на это значение.","metadata":{}},{"cell_type":"code","source":"df['price_range'] = df['price_range'].fillna('$$ - $$$')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Так как признак price_range является последовательным, т.е. каждое значение может быть сравнено с другими значениями, имеет смысл закодировать значения признака в числовое представление, с которым сможет работать алгоритм машинного обучения.\n\nДля этого создадим соответствующий словарь, в котором ключ - ценовой сегмент ресторана в старом представлении, а значение - закодированное значение ценового сегмента.","metadata":{}},{"cell_type":"code","source":"price_dict = {'$' : 1, '$$ - $$$' : 2, '$$$$' : 3}\ndf['price_range'] = df['price_range'].map(price_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Признак обработан успешно.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='2.3'></a> \n## Этап 2.3. Признак \"number_of_reviews\".\n\nВ признаке number_of_reviews также присутствуют пропуски.\n\nВ данном случае отсутствие значения в признаке, характеризующем количество ревью для определённого ресторана, резонно предположить, что отзывов у ресторана нет. Поэтому пропущенные значения заменяем на 0.","metadata":{}},{"cell_type":"code","source":"df['number_of_reviews'] = df['number_of_reviews'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df['number_of_reviews'].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пропуски успешно обработаны.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Вывод по этапу 2.\n\nНа данном этапе мы осуществили предварительную обработку данных по следующим направлениям:\n\n1. В признаке cuisine_style пропущенные значения заменили на наиболее часто встречающуюся в ресторанах кухню - Vegetarian Friendly.\n\n2. В признаке price_range пропущенные значения заменили на наиболее часто встречающийся ценовой сегмент ресторана в наборе данных, а также закодировали категориальный признак в цифровое представление.\n\n3. В признаке number_of_reviews пропущенные значения заменили на 0.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Этап 3. Исследовательский анализ данных.\n\nНа данном этапе проведём исследовательский анализ данных, чтобы выявить интересные закономерности в данных. ","metadata":{}},{"cell_type":"markdown","source":"<a id='3.1'></a> \n## Этап 3.1. Признак \"restaurant_id\".\n\nСам по себе признак является бесполезным и не будет участвовать в обучении модели, однако из этапа 1 было видно, что один и тот же ресторан встречается в наборе данных несколько раз.","metadata":{}},{"cell_type":"code","source":"df['restaurant_id'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Из этого можно предположить, что в наборе данных есть рестораны, представленные в нескольких городах.","metadata":{}},{"cell_type":"code","source":"df.groupby('restaurant_id')['city'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Так оно и оказалось. Это может стать дополнительным признаком для модели: например, признак примет значение 1, если это сеть ресторанов, и 0, если ресторан только один.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='3.2'></a> \n## Этап 3.2. Признак \"ranking\".\n\nИсследуем признак ranking.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\ndf['ranking'].hist(bins=100);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что распределение данных имеет длинный правый хвост. Это говорит о существенном дисбалансе в значениях признака. Это может быть связано с тем, что в небольших городах присутствует малое количество ресторанов, что оказывает влияние на распределение этого признака. Проверим.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nfor city in (df['city'].value_counts())[:10].index:\n    df['ranking'][df['city'] == city].hist(bins=100)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что распределение стало более однородным, однако границы распределения меняются в зависимости от города. Таким образом, подтверждаем гипотезу о том, что размер города влияет на распределение переменной ranking.\n\nЕсть две идеи о нивелировании этого воздействия.\n\nВо-первых, этот признак стоит нормировать через z-преобразование.\n\nВо-вторых, модель корректно учитывала данный признак для стран с разной величиной, его стоит забинить. Однако это осуществим на этапе создания новых признаков.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='3.3'></a> \n## Этап 3.3. Признак \"city\".\n\nИсследуем признак city. Прежде всего интересует, как количество ресторанов отличается от города к городу.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\ndf.groupby('city')['restaurant_id'].nunique().sort_values(ascending=True).plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видим, распределение ресторанов по городам также далеко от нормального. Это объясняет дисбаланс распределения данных в признаке ranking. \n\nВ качестве дополнительных признаков можно сделать дамми-кодирование признака city.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='3.4'></a> \n## Этап 3.4. Признак \"rating\".\n\nРассмотрим переменную rating.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\ndf['rating'].value_counts(ascending=False).plot(kind='barh');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что в наборе данных больше всего собрано информации о ресторанах с высокими оценками (от 3 и выше), и с оценками 0. Может быть, оценка 0 означает, что отзыв мог быть не проставлен. \n\nМеньше всего отзывов собрано для ресторанов с оценками > 0 и < 3. Вероятно, именно в этих местах модель будет ошибаться чаще.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Вывод по этапу 3.\n\nНа данном этапе мы провели исследовательский анализ данных некоторых имеющихся признаков.\n\nУдалось выяснить, что:\n\n1. В наборе данных встрачаются как одиночные рестораны, так и сети ресторанов.\n\n2. Распределение признака Ranking сильно зависит от размера города и количества в нём ресторанов.\n\n3. В разных городах имеется разное количество уникальных ресторанов.\n\n4. В наборе данных больше всего информации об отзывах с оценкой от 3 и выше, а также с оценкой 0.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Этап 4. Создание новых признаков.\n\nНа данном этапе создадим новые признаки из имеющихся.","metadata":{}},{"cell_type":"markdown","source":"<a id='4.1'></a> \n## Этап 4.1. Признак \"cuisine_style\".\n\nПоработаем с кухнями. \n\nДля начала создадим признак, который показывает, сколько разных кухонь представлено в ресторане. При этом пропущенные значения мы заменяли на значение Vegetarian Friendly, поэтому в случае наличия толькой такой кухни будем присваивать значение 1. В остальных случаях посчитаем количество кухонь в списке.","metadata":{}},{"cell_type":"code","source":"df['cuisine_count'] = df['cuisine_style'].apply(lambda x: len(x.split(',')) if x != 'Vegetarian Friendly' else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Также создадим признак, показывающий, есть ли среди кухонь ресторана наиболее популярная - Vegetarian Friendly: если есть, присвоим признаку значение 1, если нет - 0.","metadata":{}},{"cell_type":"code","source":"df['have_top_cuisine'] = df['cuisine_style'].apply(lambda x: 1 if top_cuisine in x else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Новый признак успешно создан.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.2'></a> \n## Этап 4.2. Признак \"restaurant_id\".\n\nРанее мы заметили, что в наборе данных присутствуют как одиночные рестораны, так и сети ресторанов.\n\nСоздадим признак, который это покажет: если ресторан уникальный, присвоим значение 0, если сеть - значение 1.\n\nДля начала узнаем, какие рестораны встречаются в наборе данных только 1 раз. Сформируем список таких ресторанов.","metadata":{}},{"cell_type":"code","source":"unique_restaurants = df['restaurant_id'].value_counts()[df['restaurant_id'].value_counts() == 1].index.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее создадим новый признак: если ресторан присутствует в списке уникальных ресторанов, присваиваем признаку значение 0, в противном случае - значение 1.","metadata":{}},{"cell_type":"code","source":"df['net_or_unique'] = df['restaurant_id'].apply(lambda x: 0 if x in unique_restaurants else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Новый признак успешно создан.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.3'></a> \n## Этап 4.3. Признак \"reviews\".\n\nПожалуй, самый необычный признак. Согласно описанию, данный признак содержит даты двух последних ревью. \n\nИзвлечём эти даты с помощью регулярных выражений.","metadata":{}},{"cell_type":"code","source":"pattern = re.compile(\"\\d+\\/\\d+\\/\\d+\")\ndf['dates_of_reviews'] = df['reviews'].apply(lambda x: pattern.findall(str(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Мы получили список, содержащий даты последних ревью. Однако на текущий момент эти даты представлены в виде строки. Чтобы с ними было удобно работать, преобразуем их в формат datetime.","metadata":{}},{"cell_type":"code","source":"df['dates_of_reviews'] = df['dates_of_reviews'].apply(lambda x: [pd.to_datetime(i).date() for i in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Модно заметить, что иногда в отзыве нет даты, иногда она одна. Стоит добавить признак, который покажет, сколько у нас имеется датированных отзывов.","metadata":{}},{"cell_type":"code","source":"df['dates_of_reviews_count'] = df['dates_of_reviews'].apply(lambda x: len(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на распределение признака.","metadata":{}},{"cell_type":"code","source":"df['dates_of_reviews_count'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Чаще всего встречается ситуация с двумя датированными отзывами. А вот наличие трёх датированных отзывов уже не стыкуется с описанием данных. Жить не мешает, но всё же посмотрим на них.","metadata":{}},{"cell_type":"code","source":"df[df['dates_of_reviews_count'] == 3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видим, третий отзыв всегда не является свежее двух предыдущих, что нам на руку.\n\nБудет полезно создать признаки с первым и вторым отзывом, чтобы сравнить их между собой. При этом важно учесть, что порядок нахождения отзыва в списке не говорит о том, какой отзыв был оставлен раньше, а какой позже. Поэтому даты придётся ещё и сравнивать. Для этого воспользуемся подготовленной функцией date_processing. Она учитывает наличие и только одного отзыва, и отсутствие отзыва в принципе, и наличие трёх отзывов.","metadata":{}},{"cell_type":"code","source":"df = df.apply(date_processing, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Даты успешно обработали. Теперь можно посчитать разницу в днях между первым и вторым отзывами.","metadata":{}},{"cell_type":"code","source":"df['review_timedelta'] = (pd.to_datetime(df['first_review']) - pd.to_datetime(df['second_review'])).dt.days","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, как распределён признак.","metadata":{}},{"cell_type":"code","source":"df['review_timedelta'].value_counts().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что в признаке есть как небольшие разбежки, так и неадекватно большие. Последние, вероятно, возникли из-за неправильного ввода данных. Удалять их необязательно, но и оставить тоже не хотелось бы. Решение - применить биннинг. Количество групп - 10 - было подобрано эмпирически.","metadata":{}},{"cell_type":"code","source":"df['review_timedelta'] = pd.cut(df['review_timedelta'], 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Биннинг провели, однако в таком виде их в модель не передашь. Нужно закодировать. Для этого создадим словарь и с его помощью осуществим кодировку.","metadata":{}},{"cell_type":"code","source":"interval_dict = df['review_timedelta'].value_counts().to_dict()\ninterval_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пока не очень похоже на то, что нужно: словарь есть, но его значения совсем не подходят под наши потребности. Немного исправим словарь с помощью цикла.","metadata":{}},{"cell_type":"code","source":"for i, j in enumerate(interval_dict):\n    interval_dict[j] = i\ninterval_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Гораздо лучше. Можно осуществлять кодировку.","metadata":{}},{"cell_type":"code","source":"df['review_timedelta'] = df['review_timedelta'].map(interval_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Также у нас появились признаки-даты, из которых до кучи можно извлечь пару-тройку признаков: день, месяц, год и день недели отзыва.","metadata":{}},{"cell_type":"code","source":"df['day_first_review'] = df['first_review'].dt.day\ndf['month_first_review'] = df['first_review'].dt.month\ndf['year_first_review'] = df['first_review'].dt.year\ndf['day_of_week_first_review'] = df['first_review'].dt.dayofweek\ndf['day_second_review'] = df['second_review'].dt.day\ndf['month_second_review'] = df['second_review'].dt.month\ndf['year_second_review'] = df['second_review'].dt.year\ndf['day_of_week_second_review'] = df['second_review'].dt.dayofweek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что мы успешно создали ряд новых признаков на основе дат первого и второго ревью.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.4'></a> \n## Этап 4.4. Признак \"city\".\n\nПоработаем с признаком city. Ранее мы выяснили, что количество ресторанов в городе может быть различным. Сделаем для каждого города признак, описывающий количество уникальных ресторанов для этого города.","metadata":{}},{"cell_type":"code","source":"restaurants_count_dict = df.groupby('city')['restaurant_id'].nunique().to_dict()\ndf['restaurants_in_city_count'] = df['city'].map(restaurants_count_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее из данного признака сделаем дамми-переменные, при этом - чтобы не попасть в ловушку дамми-переменных из-за эффекта мультиколлинеарности - исключим из спектра дамми-переменных одно значение признака. То есть при n-городов у нас останется n-1 дамми-признак. Зная n-1 значений соответствующих дамми-признаков, без труда можно восстановить последний дамми-признак.","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(data=df, columns=['city'], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Дамми-признаки на основе признака \"city\" успешно созданы.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.5'></a> \n## Этап 4.5. Признаки \"ranking\" и \"number_of_reviews\".\n\nПоработаем с признаками ranking и number_of_reviews.\n\nРанее мы говорили о необходимости стандартизации значений признака ranking и применения к нему биннинга. Стандартизировать можно также и признак number_of_reviews.\n\nНачнём с биннинга. Процедуру уже делали с признаком review_timedelta, делаем то же самое.","metadata":{}},{"cell_type":"code","source":"df['ranking_bins'] = pd.cut(df['ranking'], 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее кодируем значение нового признака.","metadata":{}},{"cell_type":"code","source":"ranking_bins_dict = df['ranking_bins'].value_counts().to_dict()\nranking_bins_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, j in enumerate(ranking_bins_dict):\n    ranking_bins_dict[j] = i\nranking_bins_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Кодируем.","metadata":{}},{"cell_type":"code","source":"df['ranking_bins'] = df['ranking_bins'].map(ranking_bins_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее осуществим z-стандартизацию. Будем использовать соответствующий инструмент из библиотеки sklearn. Для чистоты эксперимента обучать инструмент будем только на обучающей выборке, и на её основе трансформировать тестовую выборку.\n\nДля начала разделим их.","metadata":{}},{"cell_type":"code","source":"train = df[df['sample'] == 1]\ntest = df[df['sample'] == 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим объект инструмента шкалирования и обучим его на признаках \"ranking\" и \"number_of_reviews\" из обучающей выборки.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(train[['ranking', 'number_of_reviews']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Инструмент обучен. Применим его к данным - для обоих выборок, так как мы только обучили инструмент, но не преобразовывали обучающую выборку.","metadata":{}},{"cell_type":"code","source":"train[['ranking', 'number_of_reviews']] = scaler.transform(train[['ranking', 'number_of_reviews']])\ntest[['ranking', 'number_of_reviews']] = scaler.transform(test[['ranking', 'number_of_reviews']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Соединим наши выборки.","metadata":{}},{"cell_type":"code","source":"df = test.append(train, sort=False).reset_index(drop=True) # объединяем","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на результат.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Трансформация данных прошла хорошо.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"<a id='4.6'></a> \n## Этап 4.6. Отбор признаков.\n\nМы создали несколько новых признаков.\n\nИзбавимся от лишних признаков, которые не будут участвовать в обучении модели.","metadata":{}},{"cell_type":"code","source":"df.drop(['restaurant_id', 'reviews', 'url_ta', 'id_ta', 'cuisine_style', 'dates_of_reviews', 'first_review', \n         'second_review'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на корреляционную матрицу и избавимся от скоррелированных признаков.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(df.drop(['sample'], axis=1).corr(),cmap=\"BrBG\")\ndf.drop(['sample'], axis=1).corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видим, в наборе данных нет сильно скоррелированных с целевой переменной признаков, эффект мультиколлинеарности также не наблюдается.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Вывод по этапу 4.\n\nНа данном этапе мы создали новые признаки на основе уже имеющихся в наборе данных:\n\n1. На основе признака \"cuisine_style\" мы создали признак, показывающий количество кухонь, представленных в ресторане, а также признак наличия в ресторане самой популярной кухни.\n\n2. На основе признака \"restaurant_id\" мы создали признак, показывающий, чем является ресторан - уникальные заведением или сетью ресторанов.\n\n3. На основе признака \"reviews\" мы создали два промежуточных параметра - дата первого и второго доступных ревью - и на их основе создали ряд новых признаков: разницу в днях между ревью, а также признаки-даты из этих двух дат: год, месяц, день и день недели первого и второго ревью.\n\n4. С помощью признаков \"city\" и \"restaurant_id\" мы создали признак, описывающий количество уникальных ресторанов в каждом городе, а также дамми-переменные на основе признака \"city\", при этом для избежания эффекта мультиколлиреарности мы построили n-1 дамми-признак, где n - количество уникальных городов в наборе данных.\n\n5. На основе признака \"ranking\" была создана переменная, разбивающая признак \"ranking\" на 10 групп по мере возрастания значения признака. Кроме того, признаки \"ranking\" и \"number_of_reviews\" были нормированы с помощью z-преобразования.\n\n6. На последнем этапе мы избавились от ненужных для моделирования признаков, а также проверили признаки с помощью корреляционной матрицы на предмет сильной скоррелированности признаков с целеовой переменной и мультиколлинеарности.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Этап 5. Препроцессинг.","metadata":{}},{"cell_type":"markdown","source":"<a id='5.1'></a> \n## Этап 5.1. Написание функции для предобработки данных и создания новых признаков.\n\nНа данном этапе проделанные выше этапы предобработки данных и создания новых признаков обернём в единую функцию.\n\nДля начала заново создадим исходный набор данных.","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(data_dir + '/main_task.csv')\ndf_test = pd.read_csv(data_dir + 'kaggle_task.csv')\nsample_submission = pd.read_csv(data_dir + '/sample_submission.csv')\n\ndf_train['sample'] = 1\ndf_test['sample'] = 0\ndf_test['Rating'] = 0 \ndf = df_test.append(df_train, sort=False).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_preprocessing(df):\n    \n    df.columns = [str(i).lower().replace(' ', '_') for i in df.columns]\n    \n    top_cuisine = df[['cuisine_style']]\n    top_cuisine['cuisine_style'] = top_cuisine['cuisine_style'].str.split(',')\n    top_cuisine = top_cuisine['cuisine_style'].explode()\n    top_cuisine = top_cuisine.apply(lambda x: str(x).replace('[', '').replace(']', '').replace(\"'\", \"\").strip())\n    top_cuisine = top_cuisine.value_counts(ascending=False).index[0]\n    df['cuisine_style'] = df['cuisine_style'].fillna(top_cuisine)\n    \n    df['price_range'] = df['price_range'].fillna('$$ - $$$')\n    price_dict = {'$' : 1, '$$ - $$$' : 2, '$$$$' : 3}\n    df['price_range'] = df['price_range'].map(price_dict)\n    \n    df['number_of_reviews'] = df['number_of_reviews'].fillna(0)\n    \n    df['cuisine_count'] = df['cuisine_style'].apply(lambda x: len(x.split(',')) if x != 'Vegetarian Friendly' else 1)\n    \n    df['have_top_cuisine'] = df['cuisine_style'].apply(lambda x: 1 if top_cuisine in x else 0)\n    \n    unique_restaurants = df['restaurant_id'].value_counts()[df['restaurant_id'].value_counts() == 1].index.to_list()\n    df['net_or_unique'] = df['restaurant_id'].apply(lambda x: 0 if x in unique_restaurants else 1)\n    \n    pattern = re.compile(\"\\d+\\/\\d+\\/\\d+\")\n    df['dates_of_reviews'] = df['reviews'].apply(lambda x: pattern.findall(str(x)))\n    df['dates_of_reviews'] = df['dates_of_reviews'].apply(lambda x: [pd.to_datetime(i).date() for i in x])\n    df['dates_of_reviews_count'] = df['dates_of_reviews'].apply(lambda x: len(x))\n    df = df.apply(date_processing, axis=1)\n    df['review_timedelta'] = (pd.to_datetime(df['first_review']) - pd.to_datetime(df['second_review'])).dt.days\n    df['review_timedelta'] = pd.cut(df['review_timedelta'], 10)\n    interval_dict = df['review_timedelta'].value_counts().to_dict()\n    for i, j in enumerate(interval_dict):\n        interval_dict[j] = i\n    df['review_timedelta'] = df['review_timedelta'].map(interval_dict)\n    df['day_first_review'] = df['first_review'].dt.day\n    df['month_first_review'] = df['first_review'].dt.month\n    df['year_first_review'] = df['first_review'].dt.year\n    df['day_of_week_first_review'] = df['first_review'].dt.dayofweek\n    df['day_second_review'] = df['second_review'].dt.day\n    df['month_second_review'] = df['second_review'].dt.month\n    df['year_second_review'] = df['second_review'].dt.year\n    df['day_of_week_second_review'] = df['second_review'].dt.dayofweek\n    \n    restaurants_count_dict = df.groupby('city')['restaurant_id'].nunique().to_dict()\n    df['restaurants_in_city_count'] = df['city'].map(restaurants_count_dict)\n    \n    df = pd.get_dummies(data=df, columns=['city'], drop_first=True)\n    df['ranking_bins'] = pd.cut(df['ranking'], 10)\n    \n    ranking_bins_dict = df['ranking_bins'].value_counts().to_dict()\n    for i, j in enumerate(ranking_bins_dict):\n        ranking_bins_dict[j] = i\n    df['ranking_bins'] = df['ranking_bins'].map(ranking_bins_dict)\n    \n    train = df[df['sample'] == 1]\n    test = df[df['sample'] == 0]\n    \n    scaler = StandardScaler()\n    scaler.fit(train[['ranking', 'number_of_reviews']])\n    train[['ranking', 'number_of_reviews']] = scaler.transform(train[['ranking', 'number_of_reviews']])\n    test[['ranking', 'number_of_reviews']] = scaler.transform(test[['ranking', 'number_of_reviews']])\n    \n    df = test.append(train, sort=False).reset_index(drop=True)\n    df.drop(['restaurant_id', 'reviews', 'url_ta', 'id_ta', 'cuisine_style', 'dates_of_reviews', 'first_review', \n             'second_review'], axis=1, inplace=True)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Функция написана. Применим её к нашему набору данных.","metadata":{}},{"cell_type":"code","source":"df = data_preprocessing(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Данные в порядке.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Вывод по этапу 5.\n\nНа данном этапе мы собрали весь пайплайн предобработки данных и создания новых признаков в единую функцию. Это позволит быстро готовить аналогичные данные к повторному моделированию.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Этап 6. Обучение и тестирование модели.","metadata":{}},{"cell_type":"markdown","source":"<a id='6.1'></a> \n## Этап 6.1. Обучение и тестирование модели.\n\nНа данном этапе на ранее подготовленных данных обучим модель случайного леса и протестируем качество модели на метрике MAE.\n\nДля начала разделим данные на тренировочный и тестовый наборы по признаку \"sample\", который ранее мы создавали именно для этих целей.\n\nПризнаки \"sample\" и \"rating\" удаляем из тестового набора данных, так как мы вводили их искусственно для упрощения процесса обработки данных.\n\nИз обучающео набора данных удаляем только признак \"sample\", так как признак \"rating\" является целевым.","metadata":{}},{"cell_type":"code","source":"test = df[df['sample'] == 0]\ntrain = df[df['sample'] == 1]\ntrain.drop('sample', axis=1, inplace=True)\ntest.drop(['rating', 'sample'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Перед отправокй ответов на Kaggle протестируем работу модели на обучающей выборке. Для этого обучающую выборку разделим на обучающую и валидационную. \n\nСначала выделим признаки для обучения и целевой признак в отдельные наборы данных.","metadata":{}},{"cell_type":"code","source":"features = train.drop(['rating'], axis=1)\ntarget = train['rating']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Далее обучающую выборку разделим на обучающую и валидационную в соотношении 80/20 соответственно.","metadata":{}},{"cell_type":"code","source":"features_train, features_validation, target_train, target_validation = train_test_split(features, target, test_size=0.20, random_state=RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Приступим к работе с моделью. \n\nОбучим модель на обучающей выборке, протестируем её работу на валидационной выборке и сравним ответы модели с истинными значениями.","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\nmodel.fit(features_train, target_train)\npredictions = model.predict(features_validation)\nprint('MAE:', round(metrics.mean_absolute_error(target_validation, predictions), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нам удалось добиться неплохого результата.\n\nПосмотрим на значимость признаков с точки зрения модели.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=features_train.columns)\nfeat_importances.nlargest(15).plot(kind='barh');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наиболее значимым признаком в модели оказался ranking, следом за ним идут количество уникальных ресторанов в городе и количество ревью. Остальные признаки имеют относительно невысокую значимость для модели.\n\nТеперь для обучения модели используем всю обучающую выборку, протестируем её работу на тестовой выборке и отправим ответы на Kaggle.\n\nДля начала проверим имеющиеся выборки.","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Данные в порядке. Разделим тренировочную выборку на признаки для обучения и целевой признак.","metadata":{}},{"cell_type":"code","source":"features_train = train.drop('rating', axis=1)\ntarget_train = train['rating']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучим модель.","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\nmodel.fit(features_train, target_train)\npredictions = model.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на получившиеся предикты.","metadata":{}},{"cell_type":"code","source":"predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Заменим полученными предиктами значения рейтинга в наборе данных sample_submission.","metadata":{}},{"cell_type":"code","source":"sample_submission['Rating'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим, что получилось.","metadata":{}},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выглядит как то, что нужно. Можно отправлять ответы на Kaggle.","metadata":{}},{"cell_type":"code","source":"sample_submission.to_csv('submission_1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Вывод по этапу 6.\n\nНа данном этапе мы обучили модель и протестировали её работу с помощью метрики MAE: удалось достигнуть значения метрики в 0.2076 на валидационной выборке.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}},{"cell_type":"markdown","source":"# Итоговый вывод по проекту.\n\nВ рамках данного проекта требовалось предсказать рейтинг ресторана в TripAdvisor. Работу над проектом проводили по следующим этапам:\n\n1. Импорт библиотек, подготовка функций, чтение и первичный анализ данных.\n\n2. Предобработка данных.\n\n3. Исследовательский анализ данных.\n\n4. Создание новых признаков.\n\n5. Препроцессинг.\n\n6. Обучение и тестирование модели.\n\nМетрикой качества модели была средняя квадратическая ошибка (МАЕ).\n\nОтметим некоторые выводы по каждому этапу работы.\n\n## Вывод по этапу 1.\n\nНа данном этапе мы испортировали необходимые для работы библиотеки, подготовили несколько функций для анализа и обработки данных, а также провели мини-EDA набора данных.\n\n## Вывод по этапу 2.\n\nНа данном этапе мы осуществили предварительную обработку данных по следующим направлениям:\n\n1. В признаке cuisine_style пропущенные значения заменили на наиболее часто встречающуюся в ресторанах кухню - Vegetarian Friendly.\n\n2. В признаке price_range пропущенные значения заменили на наиболее часто встречающийся ценовой сегмент ресторана в наборе данных, а также закодировали категориальный признак в цифровое представление.\n\n3. В признаке number_of_reviews пропущенные значения заменили на 0.\n\n## Вывод по этапу 3.\n\nНа данном этапе мы провели исследовательский анализ данных некоторых имеющихся признаков.\n\nУдалось выяснить, что:\n\n1. В наборе данных встрачаются как одиночные рестораны, так и сети ресторанов.\n\n2. Распределение признака Ranking сильно зависит от размера города и количества в нём ресторанов.\n\n3. В разных городах имеется разное количество уникальных ресторанов.\n\n4. В наборе данных больше всего информации об отзывах с оценкой от 3 и выше, а также с оценкой 0.\n\n## Вывод по этапу 4.\n\nНа данном этапе мы создали новые признаки на основе уже имеющихся в наборе данных:\n\n1. На основе признака \"cuisine_style\" мы создали признак, показывающий количество кухонь, представленных в ресторане, а также признак наличия в ресторане самой популярной кухни.\n\n2. На основе признака \"restaurant_id\" мы создали признак, показывающий, чем является ресторан - уникальные заведением или сетью ресторанов.\n\n3. На основе признака \"reviews\" мы создали два промежуточных параметра - дата первого и второго доступных ревью - и на их основе создали ряд новых признаков: разницу в днях между ревью, а также признаки-даты из этих двух дат: год, месяц, день и день недели первого и второго ревью.\n\n4. С помощью признаков \"city\" и \"restaurant_id\" мы создали признак, описывающий количество уникальных ресторанов в каждом городе, а также дамми-переменные на основе признака \"city\", при этом для избежания эффекта мультиколлиреарности мы построили n-1 дамми-признак, где n - количество уникальных городов в наборе данных.\n\n5. На основе признака \"ranking\" была создана переменная, разбивающая признак \"ranking\" на 10 групп по мере возрастания значения признака. Кроме того, признаки \"ranking\" и \"number_of_reviews\" были нормированы с помощью z-преобразования.\n\n6. На последнем этапе мы избавились от ненужных для моделирования признаков, а также проверили признаки с помощью корреляционной матрицы на предмет сильной скоррелированности признаков с целеовой переменной и мультиколлинеарности.\n\n## Вывод по этапу 5.\n\nНа данном этапе мы собрали весь пайплайн предобработки данных и создания новых признаков в единую функцию. Это позволит быстро готовить аналогичные данные к повторному моделированию.\n\n## Вывод по этапу 6.\n\nНа данном этапе мы обучили модель и протестировали её работу с помощью метрики MAE: удалось достигнуть значения метрики в 0.2076 на валидационной выборке.\n\n### <a href='#0'>К оглавлению.</a> ","metadata":{}}]}